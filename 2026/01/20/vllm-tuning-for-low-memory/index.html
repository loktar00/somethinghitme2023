<!DOCTYPE html>
<html lang="en" data-theme="light">
    <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-YJDK7X0K15"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-YJDK7X0K15');
    </script>
    <meta charSet="utf-8" />
    <meta name="description" content="Jason Browns Code, demos and ideas."/>
    
    <meta http-equiv="x-ua-compatible" content="ie=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@400;500;600;700&family=Fira+Code:wght@400;500&family=Merriweather:ital,wght@0,300;0,400;0,700;1,300;1,400&display=swap" rel="stylesheet">
    <link rel="icon" href="/assets/favicon.svg">
    <link rel="manifest" href="/assets/manifest.webmanifest">
    <link rel="apple-touch-icon" sizes="48x48" href="/assets/icons/icon-48x48.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/icons/icon-72x72.png">
    <link rel="apple-touch-icon" sizes="96x96" href="/assets/icons/icon-96x96.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/icons/icon-144x144.png">
    <link rel="apple-touch-icon" sizes="192x192" href="/assets/icons/icon-192x192.png">
    <link rel="apple-touch-icon" sizes="256x256" href="/assets/icons/icon-256x256.png">
    <link rel="apple-touch-icon" sizes="384x384" href="/assets/icons/icon-384x384.png">
    <link rel="apple-touch-icon" sizes="512x512" href="/assets/icons/icon-512x512.png">
    <link href="/styles/styles.css" rel="stylesheet" />
    <link href="/styles/prism.css" rel="stylesheet" />
    <script src='/js/prism.js'></script>
    <title>vLLM Tuning For Low Memory - Somethinghitme</title>
</head>    <link href="/styles/articleStyles.css" rel="stylesheet" />
  <body>
    <div class="layout">
        <header class="container">
    <nav class="site-header">
        <button id="theme-trigger" class="logo-trigger" aria-label="Toggle Dark Mode">
            <div class="bulb-container">
                <svg class="logo-icon" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                    <path d="M12,2A7,7,0,0,0,5,9C5,11.38,6.19,13.47,8,14.74V17A1,1,0,0,0,9,18H15A1,1,0,0,0,16,17V14.74C17.81,13.47,19,11.38,19,9A7,7,0,0,0,12,2M9,21a1,1,0,0,0,1,1h4a1,1,0,0,0,1-1V20H9v1Z"/>
                </svg>
                <div class="pull-string">
                    <div class="string-line"></div>
                    <div class="string-handle"></div>
                </div>
            </div>
        </button>
        <a href="/" class="logo-text">SomethingHitMe</a>
    </nav>
</header>        <main class="container">
            <article class="article-content">
                <header class="article-header">
                    <div class="article-meta-row">
                        <time datetime="2026-01-20">
                            Tuesday, Jan 20, 2026                        </time>
                        
                            <span class="meta-separator">&bull;</span>
                            <div class="article-tags">
                                
                                    <a href="/tags/llm.html" class="article-tag">llm</a>
                                
                                    <a href="/tags/ai.html" class="article-tag">ai</a>
                                
                                    <a href="/tags/tutorial.html" class="article-tag">tutorial</a>
                                
                            </div>
                        
                    </div>
                    <h1 class="article-title">vLLM Tuning For Low Memory</h1>
                </header>
                <section>
                    <p><a href="/2026/01/20/vllm-tuning-for-low-memory/images/hero.webp"><img src="/2026/01/20/vllm-tuning-for-low-memory/images/hero.webp" alt="hero image" /></a></p>
<h2 id="theproblemvramisking">The problem VRAM is King!</h2>
<p>I have two AI nodes, one is running two 5090s the other two 3090s with NVLink. They're running Proxmox and containers so it's easy to try different configs, restore backups, etc. Altogether, between these machines, I have 112GB of VRAM, that amount of VRAM allows running of a couple of smaller models or large ones split across all of them via llama.cpp using rpc and vLLM.</p>
<blockquote>
  <p>There's a big difference between vLLM and llama.cpp however, and reasons to use one over the other.</p>
</blockquote>
<h3 id="vllm">vLLM</h3>
<p>If you want fast inference supporting multiple users (or agents) at once the best option is vLLM bar none. However it comes with some drawbacks. VRAM is <em>not</em> pooled like it is in llama.cpp, and all the memory required for the model (including KV cache) are loaded right from the beginning so there are no surprises. You <em>can</em> offload to system RAM but vLLM has flaky support for it, and system RAM is generally magnitudes slower than VRAM killing your performance.</p>
<h3 id="llamacpp">llama.cpp</h3>
<p>If you're ok not having blazing speed via tensor parallel and aren't supporting many users (or agents) and are unsure how much context you're going to end up using anyway, llama.cpp is a great option. With all of this, it truly pools your VRAM meaning in my systems I can load a model right up to the 112GB VRAM limit, it also supports using system RAM by default. You still get the speed penalty but there's no hoops to jump through to get it working.</p>
<h3 id="howdoesthisrelatetoglm47flash">How does this relate to GLM-4.7-flash?</h3>
<p>vLLM is fast like mentioned, really fast. It's able to be so fast due to tensor parallelism. However with tensor parallelism you don't get pooling up to 64GB, you get 32GB (or whatever the size of your cards are). Here's where the issue comes in. Using a 30B BF16 model means it's going to take up roughly 30GB of VRAM leaving you with 2-ish for KV cache. KV cache refers to your context. There's also minor overhead that comes with certain options.</p>
<p>When I tried running the BF16 I was extremely limited, at the end of the day I did get it to work with a context size of 8000 but it wasn't pretty. I soon jumped on the NVFP4 quant that came out a few hours later giving me much more breathing room for context.</p>
<h3 id="helpfulknobsandswitchestogetthemostoutofyourvramwithvllm">Helpful knobs and switches to get the most out of your VRAM with vLLM</h3>
<p>Spoiler, my config that ended up finally working:</p>
<pre><code>export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

uv run vllm serve zai-org/GLM-4.7-Flash \
  --download-dir /mnt/models/llm \
  --kv-cache-dtype fp8 \
  --tensor-parallel-size 2 \
  --max-model-len 8000 \
  --gpu-memory-utilization 0.96 \
  --swap-space 16 \
  --enforce-eager \
  --max-num-seqs 1 \
  --tool-call-parser glm47 \
  --reasoning-parser glm45 \
  --enable-auto-tool-choice \
  --served-model-name glm-4.7-flash \
  --host 0.0.0.0 --port 8000
</code></pre>
<p>There's a few things to notice here.</p>
<ul>
<li><code>--max-num-seqs</code> I generally start out lowering this early on, default is 256, I lower to 20 or so as a starting point.</li>
<li><code>--max-model-len</code> this is the next thing to target if the model still won't load. This is one of the most important and costly settings, this controls the total context size, lowering this can help significantly.</li>
<li><code>--gpu-memory-utilization 0.96</code> Alright… model still won't load let's tweak our memory utilization and make sure we're squeezing the most out of it. This is a value to play with 0.8 to 0.98-ish.</li>
<li><code>--enforce-eager</code> This tells vLLM to run the model in PyTorch eager mode, generally it's for stabilization but can save you a few MB by avoiding compile/capture overhead and reducing annoying memory spikes and fragmentation during startup.</li>
<li><code>--kv-cache-dtype fp8</code> alright we're getting desperate, this drops kv-cache to fp8 from BF16 saving us a little space.</li>
<li><code>--swap-space</code> we're now on our last legs, we're telling vLLM to offload a bit, it's ok to admit defeat here if this doesn't work but it's worth a shot. This is generally when it's time to go to llama.cpp.</li>
</ul>
<p>And (although deprecated) I threw this in for good measure -
<code>export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True</code>
you can use this instead
<code>export PYTORCH_ALLOC_CONF=expandable_segments:True</code>.</p>
<p>This basically tells PyTorch's CUDA memory allocator to use expandable memory segments instead of using fixed chunks of VRAM too early. For example, if you have a barge with shipping containers of all different sizes without this flag we'd just carve up the barge into fixed-size chunks based on our largest container… meaning our small ones would take up the same amount of parked space. That's not good. What this does is says hey, let's not chop this up early, let's fit these in here like Tetris.</p>
<p>tldr; it allows you to use your free RAM more efficiently.</p>
<h3 id="inclosing">In closing</h3>
<p>At the end of the day like mentioned I was able to run the NVFP4 with the following config</p>
<pre><code>uv run vllm serve GadflyII/GLM-4.7-Flash-NVFP4 \
  --download-dir /mnt/models/llm \
  --kv-cache-dtype fp8 \
  --tensor-parallel-size 2 \
  --max-model-len 82000 \
  --trust-remote-code \
  --max-num-seqs 4 \
  --tool-call-parser glm47 \
  --reasoning-parser glm45 \
  --enable-auto-tool-choice \
  --served-model-name glm-4.7-flash \
  --host 0.0.0.0 --port 8000
</code></pre>
<p>Able to run with 82000 context which is plenty for the work I'm doing. At the end of the day it's important to understand the differences between vLLM and llama.cpp and not to be afraid of tweaking the settings to get one working. As models are released vLLM is generally supported first, so it's good to understand and learn if you're on the bleeding edge. Just remember all GPUs !== pooled VRAM with vLLM.</p>
<h3 id="themodels">The Models</h3>
<ul>
<li><a href="https://huggingface.co/zai-org/GLM-4.7-Flash">GLM 4.7 Flash</a></li>
<li><a href="https://huggingface.co/GadflyII/GLM-4.7-Flash-NVFP4">GLM 4.7 Flash NVFP4</a></li>
</ul>
<p>As an aside GLM 4.7 Flash is great for local development, it <em>may</em> displace my current local favorite Devstral Small, as I play with both more I may jot down my thoughts here in a future article.</p>                </section>
            </article>
            <hr/>
            <footer>
                <!-- Author -->
<section class="author-section">
    <img src="/assets/avatar.jpg" alt="Jason Brown" class="author-avatar">
    <div class="author-info">
        <h3 class="author-name">Jason Brown (Loktar)</h3>
        <p class="author-bio">
            Christian, Grandfather, Veteran, lover of everything tech.
        </p>
        <div class="social-row">
            <a href="https://github.com/loktar00" class="social-link">
                <svg viewBox="0 0 24 24"><path d="M12,2A10,10,0,0,0,2,12C2,16.42,4.87,20.17,8.84,21.5C9.34,21.58,9.5,21.27,9.5,21C9.5,20.77,9.5,20.14,9.5,19.31C6.73,19.91,6.14,17.97,6.14,17.97C5.68,16.81,5.03,16.5,5.03,16.5C4.12,15.88,5.1,15.9,5.1,15.9C6.1,15.97,6.63,16.93,6.63,16.93C7.5,18.45,8.97,18,9.54,17.76C9.63,17.11,9.89,16.67,10.17,16.42C7.95,16.17,5.62,15.31,5.62,11.5C5.62,10.39,6,9.5,6.65,8.79C6.55,8.54,6.2,7.5,6.75,6.15C6.75,6.15,7.59,5.88,9.5,7.17C10.29,6.95,11.15,6.84,12,6.84C12.85,6.84,13.71,6.95,14.5,7.17C16.41,5.88,17.25,6.15,17.25,6.15C17.8,7.5,17.45,8.54,17.35,8.79C18,9.5,18.38,10.39,18.38,11.5C18.38,15.32,16.04,16.16,13.81,16.41C14.17,16.72,14.5,17.33,14.5,18.26C14.5,19.6,14.5,20.68,14.5,21C14.5,21.27,14.66,21.59,15.17,21.5C19.14,20.16,22,16.42,22,12A10,10,0,0,0,12,2Z"/></svg>
                GitHub
            </a>
            <a href="https://twitter.com/loktar00" class="social-link">
                <svg viewBox="0 0 24 24"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
                @loktar00
            </a>
            <a href="https://codepen.io/loktar00" class="social-link">
                <svg class='codepen' viewBox="0 1 22 24" fill="none" stroke-width="2"><path transform="translate(-22 0)" d="M22 9.3L33 2l11 7.3v7.4L33 24l-11-7.3z m0 0 11 7.4 11-7.4 m0 7.4L33 9.3l-11 7.4 M33 2v7.3 m0 7.4V24" /></svg>
                CodePen
            </a>
            <a href="https://stackoverflow.com/users/322395/loktar" class="social-link">
                <svg viewBox="0 0 24 24"><path d="M18.986 21.865v-6.404h2.134V24H1.844v-8.539h2.13v6.404h15.012zM6.111 19.731H16.85v-2.137H6.111v2.137zm.259-4.852l10.48 2.189.451-2.07-10.48-2.189-.451 2.07zm1.359-5.056l9.705 4.53.903-1.95-9.705-4.53-.903 1.95zm2.715-4.785l8.217 6.855 1.359-1.62-8.217-6.855-1.359 1.62zM15.841 0l-1.791.973 6.503 5.977 1.791-.973L15.841 0z"/></svg>
                Stack Overflow
            </a>
            <a href="https://www.dwitter.net/u/loktar" class="social-link">
                <svg viewBox="0 0 24 24"><path d="M6 4v16h2V4H6zm5 0v16h2V4h-2zm5 0v16h2V4h-2z"/></svg>
                Dwitter
            </a>
        </div>
    </div>
</section>            </footer>
            <nav class="post-footer">
    <a class="footer-link" rel="prev" href="/">← All Articles</a>
    <a class="footer-link" rel="prev" href="/2026/01/16/im-not-a-real-engineering-manager-player-coach/">I'm not a 'Real' Engineering Manager, I'm a Player-Coach →</a></nav>        </main>
        <footer class="site-footer">
    <div class="footer-content">
        <p>© 2026, Built by <a href="https://github.com/loktar00">Jason Brown - Loktar</a></p>
        <div class="footer-links">
            <a href="/feed.xml" class="feed-link" title="RSS Feed">
                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg>
                RSS
            </a>
        </div>
    </div>
</footer>    </div>
    <script src='/js/main.js'></script>
  </body>
</html>
